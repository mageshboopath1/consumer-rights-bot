import sys
import os
import re

def check_safety(llm_response: str) -> str:
    """
    Scans the LLM's response for unsafe or inappropriate content.

    This is a basic implementation that checks for a predefined list of
    unwanted keywords and sensitive information patterns.

    Args:
        llm_response (str): The text generated by the LLM.

    Returns:
        str: The original response if it's safe, or a generic safety message.
    """
    # A list of keywords to flag. In a production system, this would be a more
    # sophisticated classification model or a comprehensive list.
    unsafe_keywords = ["unethical", "dangerous", "illegal", "harmful", "sensitive"]
    
    # A basic regex to detect PII like email addresses, even though the
    # initial PII filter should have caught this.
    pii_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    
    # Check for keywords
    for keyword in unsafe_keywords:
        if keyword in llm_response.lower():
            print(f"Detected unsafe keyword: {keyword}", file=sys.stderr)
            return "I'm sorry, I cannot provide a response for that query. If you believe this is an error, please try again."
            
    # Check for PII patterns
    if re.search(pii_pattern, llm_response):
        print("Detected PII in response.", file=sys.stderr)
        return "I'm sorry, I cannot provide a response that contains sensitive information."

    # If all checks pass, the response is considered safe.
    return llm_response

if __name__ == "__main__":
    # Read the full response from standard input
    llm_response = sys.stdin.read()
    
    # Run the safety check
    final_response = check_safety(llm_response)
    
    # Print the final response to standard output
    print(final_response)